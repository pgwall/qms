{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf9081c1-662f-43db-bb53-ac3dac1e2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "import networkx.algorithms.components.connected as nxacc\n",
    "import networkx.algorithms.dag as nxadag\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec83de5-93bf-4938-b33d-f55f90b32ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadOntology():\n",
    "    \"\"\"\n",
    "    Loads VNN's hierarchy as DiGraph, as well as other helpful mappings used throughout VNN code. \n",
    "    \n",
    "    Specify alternate ontology during initialization with \"ontology_file\"\n",
    "    \n",
    "    Attributes:\n",
    "    * self.ont_filepath --> filepath to the ontology file used\n",
    "    * self.genes --> list of all genes used in VNN model\n",
    "    * self.dG --> networkx.DiGraph object of the ontology. Genes will need to be added separately if desired (not needed to run VNN)\n",
    "    * self.root --> str, the name of the network's root\n",
    "    * self.term_size_map --> dict with {\"term\":n_neurons} used in model architecture\n",
    "    * self.term2genes --> dict with {\"term\":[genes]} for all genes connected to that term\n",
    "    * self.gene2terms --> dict with {\"gene\":[terms]} for all terms connected to that gene\n",
    "    * self.term2layer --> dict with {\"term\":layer_number} for all GO terms in dG. Root is layer 1, then increments up the hierarchy  \n",
    "    * self.layer2terms --> dict with {layer_number:[terms]} for all terms in that layer number\n",
    "    * self.all --> tuple of [dG, root, term_size_map, term_direct_gene_map, gene_2_term], which can be unpacked directly into VNN\n",
    "    \"\"\"\n",
    "    def __init__(self, ontology_file='ont.txt'):\n",
    "        self.data_dir = os.path.abspath('../data')\n",
    "        print('data dir', self.data_dir)\n",
    "        \n",
    "        self.ont_filepath = os.path.join(self.data_dir, ontology_file)\n",
    "\n",
    "        self.genes = self.load_genes()\n",
    "        \n",
    "        dG, root, term_size_map, term_direct_gene_map, gene_2_term = self.load()\n",
    "        \n",
    "        self.dG = dG\n",
    "        self.root = root\n",
    "        self.term_size_map = term_size_map\n",
    "        self.term2genes = term_direct_gene_map\n",
    "        self.gene2terms = gene_2_term\n",
    "        \n",
    "        term2layer, layer2terms = self.make_layer_maps()\n",
    "        self.term2layer = term2layer\n",
    "        self.layer2terms = layer2terms\n",
    "        \n",
    "        self.go2name = self.load_go_descriptions()\n",
    "        \n",
    "        self.all = (dG, root, term_size_map, term_direct_gene_map, gene_2_term)\n",
    "    \n",
    "    def load_genes(self):\n",
    "        filepath = os.path.join(self.data_dir, 'gene2ind.txt')\n",
    "        df = pd.read_csv(filepath, sep='\\t', names=['ind','gene'])\n",
    "        return [x for x in df['gene']]\n",
    "    \n",
    "    def load(self):\n",
    "        \n",
    "        dG = nx.DiGraph()\n",
    "        gene_2_term = {}\n",
    "        term_direct_gene_map = {}\n",
    "        term_size_map = {}\n",
    "\n",
    "        with open(self.ont_filepath, 'r') as file_handle:\n",
    "            gene_set = set()\n",
    "            for line in file_handle:\n",
    "                line = line.rstrip().split()\n",
    "\n",
    "                if line[2] == 'default':\n",
    "                    dG.add_edge(line[0], line[1])\n",
    "                else:\n",
    "                    if line[1] not in self.genes:\n",
    "                        continue\n",
    "\n",
    "                    if line[0] not in term_direct_gene_map:\n",
    "                        term_direct_gene_map[line[0]] = set()\n",
    "\n",
    "                    if line[1] not in gene_2_term:\n",
    "                        gene_2_term[line[1]] = set()\n",
    "\n",
    "                    # Collect the \"child\" of genes at position 0\n",
    "                    term_direct_gene_map[line[0]].add(line[1])\n",
    "\n",
    "                    gene_2_term[line[1]].add(line[0])\n",
    "\n",
    "                    gene_set.add(line[1])\n",
    "\n",
    "            print('There are', len(gene_set), 'genes')\n",
    "\n",
    "        for term in dG.nodes():\n",
    "            term_gene_set = set()\n",
    "            if term in term_direct_gene_map:\n",
    "                term_gene_set = term_direct_gene_map[term]\n",
    "\n",
    "            deslist = nxadag.descendants(dG, term)\n",
    "\n",
    "            for child in deslist:\n",
    "                if child in term_direct_gene_map:\n",
    "                    term_gene_set = term_gene_set | term_direct_gene_map[child]\n",
    "\n",
    "            if len(term_gene_set) == 0:\n",
    "                print('There are empty terms, please delete term:', term)\n",
    "                sys.exit(1)\n",
    "            else:\n",
    "                term_size_map[term] = len(term_gene_set)\n",
    "\n",
    "        leaves = [n for n in dG.nodes if dG.in_degree(n) == 0]\n",
    "\n",
    "        root = leaves[0]\n",
    "\n",
    "        uG = dG.to_undirected()\n",
    "        connected_subG_list = list(nxacc.connected_components(uG))\n",
    "\n",
    "        print('There are', len(leaves), 'roots:', leaves[0])\n",
    "        print('There are', len(dG.nodes()), 'terms')\n",
    "        print('There are', len(connected_subG_list), 'connected componenets')\n",
    "\n",
    "        if len(leaves) > 1:\n",
    "            print(\n",
    "                'There are more than 1 root of ontology. Please use only one root.')\n",
    "            sys.exit(1)\n",
    "        if len(connected_subG_list) > 1:\n",
    "            print('There are more than connected components. Please connect them.')\n",
    "            sys.exit(1)\n",
    "\n",
    "        return dG, root, term_size_map, term_direct_gene_map, gene_2_term\n",
    "    \n",
    "    def make_layer_maps(self):\n",
    "        dGl = self.dG.copy()\n",
    "\n",
    "        term2layer = {}\n",
    "        layer2terms = {}\n",
    "        i = 1\n",
    "        while True:\n",
    "            leaves = [n for n in dGl.nodes() if dGl.in_degree(n) == 0]\n",
    "\n",
    "            if len(leaves) == 0:\n",
    "                break\n",
    "\n",
    "            # leaves are only terms connected to genes\n",
    "            layer2terms[i] = leaves\n",
    "\n",
    "            for term in leaves:\n",
    "                term2layer[term] = i\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            dGl.remove_nodes_from(leaves)\n",
    "            \n",
    "        return term2layer, layer2terms\n",
    "    \n",
    "    def load_go_descriptions(self):\n",
    "        go_2_name_file = '/cellar/users/pgwall/DrugCell/data/go_term_descriptions.txt'\n",
    "        go_2_name_df = pd.read_csv(go_2_name_file, sep='\\t', names=['def', 'name', 'term'], skiprows=1)\n",
    "\n",
    "        go_2_name_file_2 = '/cellar/users/pgwall/DrugCell/Models/super_model/nest_vnn/validation/go_term_annotations.tsv'\n",
    "        go_2_name_df_2 = pd.read_csv(go_2_name_file_2, sep='\\t', names=['term','name','process','def'])\n",
    "\n",
    "        go_2_name = dict(zip(go_2_name_df['term'].tolist(), go_2_name_df['name'].tolist()))\n",
    "        go_2_name_2 = dict(zip(go_2_name_df_2['term'].tolist(), go_2_name_df_2['name'].tolist()))\n",
    "\n",
    "        go_2_name.update(go_2_name_2)\n",
    "    \n",
    "        return go_2_name\n",
    "    \n",
    "    def add_genes(self):\n",
    "        \"\"\"\n",
    "        Returns copy of self.dG annotated with self.genes \n",
    "        \"\"\"\n",
    "        dGg = self.dG.copy()\n",
    "\n",
    "        for gene, terms in self.gene2terms.items():\n",
    "            for term in terms:\n",
    "                dGg.add_edge(term, gene)\n",
    "                \n",
    "        return dGg\n",
    "    \n",
    "    def add_features(self, dG_genes, features):\n",
    "        \"\"\"\n",
    "        Returns copy of dG_genes annotated with features\n",
    "        \"\"\"\n",
    "        dGgf= dG_genes.copy()\n",
    "        \n",
    "        if not isinstance(features, list):\n",
    "            features = [features]\n",
    "            \n",
    "        gene_nodes = [x for x in dGgf.nodes() if x in self.gene2terms]\n",
    "\n",
    "        for gene in gene_nodes:\n",
    "            for feat in features:\n",
    "                feat_str = '_'.join([gene, feat])\n",
    "                dGgf.add_edge(gene, feat_str)\n",
    "                \n",
    "        return dGgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6437b0fc-aef5-4804-b183-78b283c81216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dir /cellar/users/pgwall/data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/cellar/users/pgwall/data/gene2ind.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ont \u001b[38;5;241m=\u001b[39m \u001b[43mLoadOntology\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m, in \u001b[0;36mLoadOntology.__init__\u001b[0;34m(self, ontology_file)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata dir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mont_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, ontology_file)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_genes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m dG, root, term_size_map, term_direct_gene_map, gene_2_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdG \u001b[38;5;241m=\u001b[39m dG\n",
      "Cell \u001b[0;32mIn[20], line 45\u001b[0m, in \u001b[0;36mLoadOntology.load_genes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_genes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     44\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgene2ind.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mind\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgene\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgene\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/cellar/users/pgwall/data/gene2ind.txt'"
     ]
    }
   ],
   "source": [
    "ont = LoadOntology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "617cf7c6-56a2-4e55-acd9-bf6d5e085b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/cellar/users/pgwall/qms'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a920155-319c-4300-8a6a-fc834c755585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/cellar/users/pgwall/data'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6919c85-b759-41ad-b507-adc77855a7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
